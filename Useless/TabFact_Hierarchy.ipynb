{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Hierarchy construction of TabFact- Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import deque\n",
    "import ast\n",
    "import os\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import os.path\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import io\n",
    "import requests\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# This is for querying the sub-hierarchical tree in each lowest-type\n",
    "def query_wikidata_api(wiki_url):\n",
    "    # Wikidata SPARQL endpoint URL\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "    # Define the request headers with the user agent\n",
    "    headers = {\n",
    "        \"User-Agent\": \"My-App/1.0\"\n",
    "    }\n",
    "\n",
    "    # Define the request parameters\n",
    "    sparql_query = f\"\"\"\n",
    "    SELECT ?x ?xLabel ?classLabel ?superclassLabel ?superclass2Label ?superclass3Label ?superclass4Label ?superclass5Label ?superclass6Label WHERE {{\n",
    "    <{wiki_url}> schema:about ?x.\n",
    "    ?x wdt:P31 ?class. #instance Of\n",
    "    ?class wdt:P279 ?superclass. #subclass of superclass1\n",
    "    ?superclass wdt:P279 ?superclass2. #subclass of superclass2\n",
    "    ?superclass2 wdt:P279 ?superclass3. #subclass of superclass3\n",
    "    ?superclass3 wdt:P279 ?superclass4. #subclass of superclass4\n",
    "    ?superclass4 wdt:P279 ?superclass5. #subclass of superclass5\n",
    "    ?superclass5 wdt:P279 ?superclass6. #subclass of superclass6\n",
    "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the GET request to the Wikidata API\n",
    "    response = requests.get(endpoint_url, params={\"format\": \"json\", \"query\": sparql_query}, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch data for {wiki_url}.\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Parallel querying\n",
    "\n",
    "def query_wikidata_parallel(wiki_urls):\n",
    "    # Query the Wikidata API for each URL in parallel\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results_json_list = list(executor.map(query_wikidata_api, list(wiki_urls.values())))\n",
    "    # Convert the JSON results to DataFrames\n",
    "    dataframes = []\n",
    "    for index, results_json in enumerate(results_json_list):\n",
    "        if results_json:\n",
    "            rows = []\n",
    "            for item in results_json[\"results\"][\"bindings\"]:\n",
    "                row = {\n",
    "                    \"x\": item[\"x\"][\"value\"],\n",
    "                    \"xLabel\": item[\"xLabel\"][\"value\"],\n",
    "                    \"classLabel\": item[\"classLabel\"][\"value\"],\n",
    "                    \"superclassLabel\": item[\"superclassLabel\"][\"value\"],\n",
    "                    \"superclass2Label\": item[\"superclass2Label\"][\"value\"],\n",
    "                    \"superclass3Label\": item[\"superclass3Label\"][\"value\"],\n",
    "                    \"superclass4Label\": item[\"superclass4Label\"][\"value\"],\n",
    "                    \"superclass5Label\": item[\"superclass5Label\"][\"value\"],\n",
    "                    \"superclass6Label\": item[\"superclass6Label\"][\"value\"]\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "            if len(rows):\n",
    "                df = pd.DataFrame(rows)\n",
    "                data_path = os.path.join(os.getcwd(), \"datasets/TabFact/Label\", list(wiki_urls.keys())[index])\n",
    "                df.to_csv(data_path)\n",
    "                dataframes.append(df)\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "#Start crawling\n",
    "def parallel_crawling(gt_csv:dict):\n",
    "    for i in range(0, 11):  # 160\n",
    "\n",
    "        end = (i + 1) * 400 - 1\n",
    "        if end >= len(gt_csv):\n",
    "            end = len(gt_csv)\n",
    "        start = i * 400\n",
    "        slice = gt_csv[start:end]\n",
    "\n",
    "        result_diction = dict(zip(slice.iloc[:, 0], slice.iloc[:, 2]))\n",
    "\n",
    "        dataframes_list = query_wikidata_parallel(result_diction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_max_distance(graph, start_node, target_node):\n",
    "    visited = set()\n",
    "    queue = deque([(start_node, 0)])  # (节点, 距离)\n",
    "    max_distance = -1\n",
    "\n",
    "    while queue:\n",
    "        node, distance = queue.popleft()\n",
    "\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            max_distance = max(max_distance, distance)\n",
    "\n",
    "            if node == target_node:\n",
    "                return max_distance\n",
    "\n",
    "            for successor in graph.successors(node):\n",
    "                queue.append((successor, distance + 1))\n",
    "\n",
    "    return max_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Start constructing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# the target path of TabFact data\n",
    "target_path = os.path.join(os.getcwd(), \"datasets/TabFact/\")\n",
    "\n",
    "\n",
    "abstract = [ 'PhysicalActivity','object', 'result', 'temporal entity', 'inconsistency', 'noun', 'noun phrase', 'remains', 'use',\n",
    "            'independent continuant', 'observable entity', 'artificial entity', 'natural physical object',\n",
    "            'occurrence', 'relation', 'group of physical objects', 'economic entity', 'group of works',\n",
    "            'concrete object', 'three-dimensional object', 'part', 'geographic entity', 'artificial geographic entity',\n",
    "            'source', 'group or class of physical objects', 'role', 'phenomenon', 'physical entity', 'means',\n",
    "            'spatio-temporal entity', 'spatial entity', 'one-dimensional space', 'physical object',\n",
    "            'continuant', 'collective entity', 'space object', 'type', 'information', 'anatomical entity',\n",
    "            'output', 'abstract object', 'class', 'non-physical entity', 'integral', 'quantity', 'former entity',\n",
    "            'occurrent', 'cause', 'idiom', 'lect', 'modification', 'alteration', 'control', 'consensus',\n",
    "            'social relation', 'process', 'rivalry', 'mental process', 'condition',\n",
    "            'social phenomenon', 'manifestation', 'work', 'source of information', 'knowledge type', 'action',\n",
    "            'time interval', 'interaction', 'record', 'language variety', 'intentional human activity',\n",
    "            'status', 'group of living things', 'agent', 'sign', 'content', 'converter', 'resource', 'metaclass',\n",
    "            'unit', 'human activity','effect', 'archives', 'sub-fonds', 'evaluation',\n",
    "            'interface', 'contributing factor', 'undesirable characteristic', 'structure', 'method', 'matter', 'change',\n",
    "            'physical phenomenon', 'binary relation', 'building work', 'power', 'management', 'long, thin object',\n",
    "            'definite integral', 'physical property', 'multi-organism process', 'data', 'multiset', 'line',\n",
    "            'proper noun', 'physicochemical process', 'group', 'collection', 'historical source'\n",
    "            'interaction', 'information resource', 'list', 'plan', 'scale', 'memory', 'social structure',\n",
    "            'source text', 'open content', 'written work', 'strategy', 'group of humans', 'system', 'deformation',\n",
    "            'representation', 'multicellular organismal process', 'operator', 'social system']\n",
    "top = ['Place', 'Action', 'Intangible', 'Organization', 'CreativeWork', 'MedicalEntity', 'BioChemEntity', 'Event', 'Product', 'Person', 'Taxon']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ground_label_name1 = \"01SourceTables.csv\"\n",
    "data_path = os.path.join(os.getcwd(), \"datasets/TabFact/\", ground_label_name1)\n",
    "ground_truth_csv = pd.read_csv(data_path, encoding='latin-1')\n",
    "result_dict = dict(zip(ground_truth_csv.iloc[:, 0], ground_truth_csv.iloc[:, 2]))\n",
    "names = ground_truth_csv[\"fileName\"].unique()\n",
    "labels = os.listdir(os.path.join(os.getcwd(), \"datasets/TabFact/Label\"))\n",
    "no_labels = [i for i in names if i not in labels]\n",
    "# ground_truth_csv = ground_truth_csv[ground_truth_csv[\"fileName\"].isin(no_labels)]\n",
    "ground_truth = dict(zip(ground_truth_csv.iloc[:, 0], ground_truth_csv.iloc[:, 4]))\n",
    "\n",
    "\n",
    "similar_words = {}\n",
    "with open(\"filter_sim_all.pkl\", \"rb\") as file:\n",
    "    all_sims = pickle.load(file)\n",
    "for key, value in all_sims.items():\n",
    "    for tuple in value.keys():\n",
    "        word = tuple[0]\n",
    "        if tuple[0] in similar_words.keys():\n",
    "            if tuple[1] not in similar_words[word]:\n",
    "                similar_words[word].append(tuple[1])\n",
    "        else:\n",
    "            similar_words[word] = [tuple[1]]\n",
    "\n",
    "for word, similar_word_list in similar_words.items():\n",
    "    if len(similar_word_list) == 1:\n",
    "        similar_words[word] = similar_word_list[0]\n",
    "print(similar_words)\n",
    "#unique_items = list(set(similar_words.values()))\n",
    "\n",
    "\n",
    "\n",
    "node_length = 0\n",
    "G = nx.DiGraph()\n",
    "for index, row in ground_truth_csv.iterrows():\n",
    "    if row[\"fileName\"] in labels:\n",
    "        label_path = os.path.join(os.getcwd(), \"datasets/TabFact/Label\")\n",
    "        df = pd.read_csv(os.path.join(label_path, row[\"fileName\"]), encoding='UTF-8').iloc[:, 3:9]\n",
    "        for _, row2 in df.iterrows():\n",
    "            labels_table = row2.dropna().tolist()\n",
    "            for i in range(len(labels_table) - 1):\n",
    "                if labels_table[i + 1] != labels_table[i]:\n",
    "                    #if labels_table[i + 1] not in abstract and labels_table[i] not in abstract:\n",
    "                        child_type = labels_table[i]\n",
    "                        if labels_table[i + 1] in G.nodes():\n",
    "                                if labels_table[i] not in nx.ancestors(G, labels_table[i + 1]):\n",
    "                                            G.add_edge(labels_table[i + 1], child_type)\n",
    "                                            continue\n",
    "                        else:\n",
    "                                        G.add_edge(labels_table[i + 1], child_type)\n",
    "                                        continue\n",
    "    else:\n",
    "        if row[\"class\"] != \" \":\n",
    "            superclass = row[\"class\"]\n",
    "            classX = row[\"superclass\"]\n",
    "            all_nodes = {superclass, classX}\n",
    "            all_nodes = all_nodes - set(G.nodes())\n",
    "            G.add_nodes_from(all_nodes)\n",
    "            G.add_edge(superclass,classX)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Dump hierarchy\n",
    "\n",
    "\n",
    "with open(os.path.join(target_path, \"graphGroundTruth2.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(G, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The following is for the combining the schema.org\n",
    "\"\"\"\n",
    "for index, row in ground_truth_csv.iterrows():\n",
    "    if row[\"fileName\"] in labels:\n",
    "        label_path = os.path.join(os.getcwd(), \"datasets/TabFact/Label\")\n",
    "        df = pd.read_csv(os.path.join(label_path, row[\"fileName\"]), encoding='UTF-8').iloc[:, 3:9]\n",
    "        for _, row2 in df.iterrows():\n",
    "            labels_table = row2.dropna().tolist()\n",
    "            for i in range(len(labels_table) - 1):\n",
    "                if labels_table[i + 1] != labels_table[i]:\n",
    "                    if labels_table[i + 1] not in abstract and labels_table[i] not in abstract:\n",
    "                        child_type = similar_words[labels_table[i]] \\\n",
    "                            if labels_table[i] in similar_words.keys() else labels_table[i]\n",
    "                        if child_type in top:\n",
    "                            break\n",
    "                        else:\n",
    "                            if labels_table[i + 1] in G.nodes():\n",
    "                                if labels_table[i] not in nx.ancestors(G, labels_table[i + 1]):\n",
    "                                    if labels_table[i + 1] not in similar_words.keys():\n",
    "                                        if labels_table[i + 1] != child_type \\\n",
    "                                                and \"process\" not in labels_table[i + 1].lower() \\\n",
    "                                                and \"process\" not in child_type.lower():\n",
    "                                            G.add_edge(labels_table[i + 1], child_type)\n",
    "\n",
    "                                            continue\n",
    "                                    else:\n",
    "                                        if similar_words[labels_table[i + 1]] != child_type and \"process\" not in \\\n",
    "                                                labels_table[i + 1].lower() \\\n",
    "                                                and \"process\" not in child_type.lower():\n",
    "                                            G.add_edge(similar_words[labels_table[i + 1]], child_type)\n",
    "                                            break\n",
    "                            else:\n",
    "                                if labels_table[i + 1] not in similar_words.keys():\n",
    "                                    if labels_table[i + 1] != child_type and \"process\" not in labels_table[\n",
    "                                        i + 1].lower() \\\n",
    "                                            and \"process\" not in child_type.lower():\n",
    "                                        G.add_edge(labels_table[i + 1], child_type)\n",
    "                                        continue\n",
    "                                else:\n",
    "                                    if similar_words[labels_table[i + 1]] != child_type and \"process\" not in \\\n",
    "                                            labels_table[i + 1].lower() \\\n",
    "                                            and \"process\" not in child_type.lower():\n",
    "                                        G.add_edge(similar_words[labels_table[i + 1]], child_type)\n",
    "                                        break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read the hierarchy\n",
    "We first detect what is in the top level\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/CurrentDataset/datasets/TabFact/graphGroundTruth2.pkl\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mEOFError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(file_path)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(file_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m----> 5\u001B[0m     G \u001B[38;5;241m=\u001B[39m \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     Top_level_nodes \u001B[38;5;241m=\u001B[39m [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m G\u001B[38;5;241m.\u001B[39mnodes \u001B[38;5;28;01mif\u001B[39;00m G\u001B[38;5;241m.\u001B[39min_degree(i) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(Top_level_nodes,\u001B[38;5;28mlen\u001B[39m(Top_level_nodes))\n",
      "\u001B[0;31mEOFError\u001B[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "\n",
    "target_path = os.path.join(os.getcwd(), \"datasets/TabFact/\")\n",
    "file_path = os.path.join(os.path.join(target_path, \"graphGroundTruth2.pkl\"))\n",
    "print(file_path)\n",
    "with open(file_path, \"rb\") as file:\n",
    "    G = pickle.load(file)\n",
    "    Top_level_nodes = [i for i in G.nodes if G.in_degree(i) == 0]\n",
    "print(Top_level_nodes,len(Top_level_nodes))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = os.listdir(os.path.join(os.getcwd(), \"datasets/TabFact/Label\"))\n",
    "ground_label_name = \"01SourceTables.csv\"\n",
    "data_path = os.path.join(os.getcwd(), \"datasets/TabFact/\", ground_label_name)\n",
    "ground_truth_csv = pd.read_csv(data_path, encoding='latin-1')\n",
    "for index, row in ground_truth_csv.iterrows():\n",
    "    if row[\"fileName\"] in labels:\n",
    "        label_path = os.path.join(os.getcwd(), \"datasets/TabFact/Label\")\n",
    "        df = pd.read_csv(os.path.join(label_path, row[\"fileName\"]), encoding='UTF-8').iloc[:, 3:9]\n",
    "        lowest_types = df.iloc[:, 0].unique()\n",
    "        top_level_types = []\n",
    "        for type_low in lowest_types:\n",
    "            if type_low in G.nodes():\n",
    "                parent_top_per = [item for item in nx.ancestors(G, type_low) if G.in_degree(item) == 0]\n",
    "                for top_per in parent_top_per:\n",
    "                    if top_per not in top_level_types:\n",
    "                        top_level_types.append(top_per)\n",
    "        ground_truth_csv.iloc[index, 4] = lowest_types\n",
    "        ground_truth_csv.iloc[index, 5] = top_level_types\n",
    "ground_truth_csv.to_csv(os.path.join(target_path, \"new_test_origin.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
